








<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>Jupyter Notebook Viewer</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">
  
  <meta name="robots" content="noindex,nofollow">
  

  <!--NEW RELIC Start Perf Measurement-->
  
  <!--NREND-->

  <!-- Le styles -->
  <script src="/cdn-cgi/apps/head/MuIIl4I_IVFkxldaVu1mdWee9as.js"></script><link href="/static/build/styles.css?v=d2f574aebb7720ccb35207ba1db4cfbb" rel="stylesheet">

  <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->

  <!-- Le fav and touch icons -->
  <link rel="shortcut icon" href="/static/ico/ipynb_icon_16x16.png">
  <link rel="apple-touch-icon-precomposed" sizes="144x144"
        href="/static/ico/apple-touch-icon-144-precomposed.png?v=5a3c9ede93e2a8b8ea9e3f8f3da1a905">
  <link rel="apple-touch-icon-precomposed" sizes="114x114"
        href="/static/ico/apple-touch-icon-114-precomposed.png?v=45d86fc8f24dc00638035e1dd7a6d898">
  <link rel="apple-touch-icon-precomposed" sizes="72x72"
        href="/static/ico/apple-touch-icon-72-precomposed.png?v=540b5eb0f3cfd25f1439d1c9bd30e15f">
  <link rel="apple-touch-icon-precomposed"
        href="/static/ico/apple-touch-icon-57-precomposed.png?v=225f0590e187e1458625654f10a28f56">
  
  

  

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Notebook on nbviewer">
  <meta name="twitter:description" content="Check out this Jupyter notebook!">

  
  <meta name="twitter:domain" content="nbviewer.ipython.org">
  <meta name="twitter:image:src" content="http://ipython.org/ipython-doc/dev/_images/ipynb_icon_128x128.png">

  
    <link href="/static/build/notebook.css?v=7ff1df8ccee6dd13babbd66843869949" rel="stylesheet">
  

  

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML" type="text/javascript">
    </script>
    <script type="text/javascript">
      init_mathjax = function() {
        if (window.MathJax) {
          // MathJax loaded
          MathJax.Hub.Config({
            TeX: {
              equationNumbers: {
                autoNumber: "AMS",
                useLabelIds: true
              }
            },
            tex2jax: {
              inlineMath: [ ['$','$'], ["\\(","\\)"] ],
              displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
              processEscapes: true,
              processEnvironments: true
            },
            displayAlign: 'center',
            "HTML-CSS": {
              styles: {'.MathJax_Display': {"margin": 0}},
              linebreaks: { automatic: true }
            }
          });
          MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
      }
      init_mathjax();
    </script>
  

  
    <script>
      (function() {
        function addWidgetsRenderer() {
          var mimeElement = document.querySelector('script[type="application/vnd.jupyter.widget-view+json"]');
          var scriptElement = document.createElement('script');
          var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js';
          var widgetState;

          try {
            widgetState = mimeElement && JSON.parse(mimeElement.innerHTML);

            if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) {
              widgetRendererSrc = 'https://unpkg.com/jupyter-js-widgets@*/dist/embed.js';
            }
          } catch(e) {}

          scriptElement.src = widgetRendererSrc;
          document.body.appendChild(scriptElement);
        }

        document.addEventListener('DOMContentLoaded', addWidgetsRenderer);
      }());
    </script>
  

</head>

<body class="nbviewer">

  <!-- These are loaded at the top of the body so they are available to
       notebook cells when they are loaded below. -->
  <script src="/static/components/jquery/dist/jquery.min.js?v=c9f5aeeca3ad37bf2aa006139b935f0a"></script>
  <script src="/static/components/requirejs/require.js?v=6da8be361b9ee26c5e721e76c6d4afce"></script>
  <script src="/static/components/moment/min/moment.min.js?v=89f87298ad94aa1e6b92f42eb66da043"></script>
<!-- Navbar
================================================== -->
  <nav id="menubar" class="navbar navbar-default navbar-fixed-top" data-spy="affix">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse">
          <span class="sr-only">Toggle navigation</span>
          <i class="fa fa-bars"></i>
        </button>
        <a class="navbar-brand" href="/">
          <img src="/static/img/nav_logo.svg?v=479cefe8d932fb14a67b93911b97d70f" width="159"/>
        </a>
      </div>

      <div class="collapse navbar-collapse">
        <ul class="nav navbar-nav navbar-right">
          <li>
            <a class="active" href="http://jupyter.org">JUPYTER</a>
          </li>
          <li>
    <a href="/faq" title="FAQ" >
      
        <span>FAQ</span>
      
    </a>
  </li>

          
  
    
  
    
      
        <li>
    <a href="/format/script/url/www.cs.colostate.edu/~anderson/cs445/notebooks/14%20Classification%20with%20Linear%20Logistic%20Regression.ipynb" title="View as Code" >
      <span class="fa fa-code fa-2x menu-icon"></span>
      <span class="menu-text">View as Code</span>
    </a>
  </li>
      
    
  

  
    <li>
    <a href="#" title="Python 3 Kernel" >
      <span class="fa fa-server fa-2x menu-icon"></span>
      <span class="menu-text">Python 3 Kernel</span>
    </a>
  </li>
  

  

  <li>
    <a href="http://www.cs.colostate.edu/%7Eanderson/cs445/notebooks/14%20Classification%20with%20Linear%20Logistic%20Regression.ipynb" title="Download Notebook" download>
      <span class="fa fa-download fa-2x menu-icon"></span>
      <span class="menu-text">Download Notebook</span>
    </a>
  </li>

        </ul>
      </div><!-- /.navbar-collapse -->
      
      
    </div>
  </nav>

  <div class="container container-main">
    
  
  <div id="notebook">
    <div id="notebook-container">
      
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$$\newcommand{\xv}{\mathbf{x}}
\newcommand{\Xv}{\mathbf{X}}
\newcommand{\yv}{\mathbf{y}}
\newcommand{\Yv}{\mathbf{Y}}
\newcommand{\zv}{\mathbf{z}}
\newcommand{\av}{\mathbf{a}}
\newcommand{\Wv}{\mathbf{W}}
\newcommand{\wv}{\mathbf{w}}
\newcommand{\betav}{\mathbf{\beta}}
\newcommand{\gv}{\mathbf{g}}
\newcommand{\Hv}{\mathbf{H}}
\newcommand{\dv}{\mathbf{d}}
\newcommand{\Vv}{\mathbf{V}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\tv}{\mathbf{t}}
\newcommand{\Tv}{\mathbf{T}}
\newcommand{\Sv}{\mathbf{S}}
\newcommand{\zv}{\mathbf{z}}
\newcommand{\Zv}{\mathbf{Z}}
\newcommand{\Norm}{\mathcal{N}}
\newcommand{\muv}{\boldsymbol{\mu}}
\newcommand{\sigmav}{\boldsymbol{\sigma}}
\newcommand{\phiv}{\boldsymbol{\phi}}
\newcommand{\Phiv}{\boldsymbol{\Phi}}
\newcommand{\Sigmav}{\boldsymbol{\Sigma}}
\newcommand{\Lambdav}{\boldsymbol{\Lambda}}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\argmax}[1]{\underset{#1}{\operatorname{argmax}}}
\newcommand{\argmin}[1]{\underset{#1}{\operatorname{argmin}}}
\newcommand{\dimensionbar}[1]{\underset{#1}{\operatorname{|}}}
\newcommand{\dimensionbar}[1]{\underset{#1}{\operatorname{|}}}
\newcommand{\grad}{\mathbf{\nabla}}
\newcommand{\ebx}[1]{e^{\wv_{#1}^T \xv_n}}
\newcommand{\eby}[1]{e^{y_{n,#1}}}
\newcommand{\Tiv}{\mathbf{Ti}}
\newcommand{\Fv}{\mathbf{F}}
\newcommand{\ones}[1]{\mathbf{1}_{#1}}
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Classification-with-Linear-Logistic-Regression">Classification with Linear Logistic Regression<a class="anchor-link" href="#Classification-with-Linear-Logistic-Regression">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Motivation-and-Setup">Motivation and Setup<a class="anchor-link" href="#Motivation-and-Setup">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Recall that a linear model used for classification can result in masking. We discussed fixing this by using different
shaped membership functions, other than linear.</p>
<p>Our first approach to this was to use generative models (Normal distributions) to model the data
from each class, forming $p(\xv|C=k)$.  Using Bayes Theorem, we converted this to $p(C=k|\xv)$ and
derived QDA and LDA.</p>
<p>Now we will derive a linear model that directly predicts $p(C=k|\xv)$, resulting in the algorithm called logisitic
regression.  It is derived to maximize the likelihood of the data, given a bunch of samples and their class labels.</p>
<p>Remember this picture?</p>
<p><img src="http://www.cs.colostate.edu/~anderson/cs445/notebooks/indicatorvarsmax2.png" width=400></p>
<p>The problem was that the green line for Class 2 was too low.
In fact, all lines are too low in the middle of x range.  Maybe we
can reduce the masking effect by</p>
<ul>
<li>requiring the function values to be between 0 and 1, and</li>
<li>requiring them to sum to 1 for every value of x.</li>
</ul>
<p>We can satisfy those two requirements by directly representing
$p(C=k|\xv)$ as</p>
<p>$$
    \begin{align*}
      p(C=k|\xv) = \frac{f(\xv;\wv_k)}{\sum_{m=1}^K f(\xv;\wv_m)}
    \end{align*}
$$</p>
<p>with $f(\xv;\wv) \ge 0$. We haven't discussed the form of $f$ yet, but $\wv$
represents the parameters of $f$ that we will tune to fit the
training data (later).</p>
<p>This is certainly an expression that is between 0 and 1 for
any $\xv$.
And we have $p(C=k|\xv)$ expressed directly, as opposed to
the previous generative approach of first modeling $p(\xv|C=k)$
and using Bayes' theorem to get $p(C=k|\xv)$.</p>
<p>Let's give the above expression another name</p>
<p>$$
    \begin{align*}
      g_k(\xv) = p(C=k|\xv) = \frac{f(\xv;\wv_k)}{\sum_{m=1}^K f(\xv;\wv_m)}
    \end{align*}
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Derivation">Derivation<a class="anchor-link" href="#Derivation">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Whatever we choose for $f$, we must make a plan for
optimizing its parameters $\wv$.  How?</p>
<p>Let's maximize the likelihood of the data.  So, what is the
likelihood of training data consisting of samples $\{\xv_1, \xv_2, \ldots, \xv_N\}$ and class indicator variables</p>
<p>$$
  \begin{align*}
    \begin{pmatrix}
      t_{1,1} &amp; t_{1,2} &amp; \ldots &amp; t_{1,K}\\
      t_{2,1} &amp; t_{2,2} &amp; \ldots &amp; t_{2,K}\\
      \vdots\\
      t_{N,1} &amp; t_{N,2} &amp; \ldots &amp; t_{N,K}
    \end{pmatrix}
  \end{align*}
$$</p>
<p>with every value $t_{n,k}$ being 0 or 1, and each row of this matrix
contains a single 1? (We can also express $\{\xv_1, \xv_2,
\ldots, \xv_N\}$ as an $N \times D$ matrix, but we will be using
single samples $\xv_n$ more often in the following.)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Data-Likelihood">Data Likelihood<a class="anchor-link" href="#Data-Likelihood">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The likelihood is just the product of all $p(C=\text{class of }
n^\text{th}\text{ sample}\,|\,\xv_n)$ values
for sample $n$.  A common way to express this product, using those handy indicator variables is</p>
<p>$$
    \begin{align*}
      L(\betav) = \prod_{n=1}^N \prod_{k=1}^K p(C=k\,|\, \xv_n)^{t_{n,k}}
    \end{align*}
$$</p>
<p>Say we have three classes ($K=3$) and training sample $n$ is from Class 2, then the  product is</p>
<p>$$
      \begin{align*}
        p(C=1\,|\,\xv_n)^{t_{n,1}} p(C=2\,|\,\xv_n)^{t_{n,2}}
        p(C=3\,|\,\xv_n)^{t_{n,3}} &amp; = 
         p(C=1\,|\,\xv_n)^0 p(C=2\,|\,\xv_n)^1 p(C=3\,|\,\xv_n)^0 \\
        &amp; = 1\; p(C=2\,|\,\xv_n)^1 \; 1 \\
        &amp; = p(C=2\,|\,\xv_n) 
      \end{align*}
$$</p>
<p>This shows how the indicator variables as exponents select the correct terms to be included in the product.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Maximizing-the-Data-Likelihood">Maximizing the Data Likelihood<a class="anchor-link" href="#Maximizing-the-Data-Likelihood">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So, we want to find $\wv$ that maximizes the data likelihood.  How shall we proceed?</p>
<p>$$
    \begin{align*}
      L(\wv) &amp; = \prod_{n=1}^N \prod_{k=1}^K p(C=k\,|\, \xv_n) ^ {t_{n,k}}
    \end{align*}
$$</p>
<p>Right.  Find the derivative with respect to each component of $\wv$, or the gradient with respect to $\wv$.  But there is
a mess of products in this. So...</p>
<p>Right again.  Work with the natural logarithm  $\log L(\wv)$ which we will call $LL(\wv)$.</p>
<p>$$
    \begin{align*}
      LL(\wv) = \log L(\wv) = \sum_{n=1}^N \sum_{k=1}^K t_{n,k}  \log p(C=k\,|\,\xv_n)
    \end{align*}
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Gradient-Ascent">Gradient Ascent<a class="anchor-link" href="#Gradient-Ascent">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Unfortunately, the gradient of $LL(\wv)$ with respect to
$\wv$ is not linear in $\wv$, so we cannot simply set the
result equal to zero and solve for $\wv$.</p>
<p>Instead, we do gradient ascent. (Why "ascent"?)</p>
<ul>
<li>Initialize $\wv$ to some value.</li>
<li>Make small change to $\wv$ in the direction of the  gradient of $LL(\wv)$ with respect to $\wv$  (or $\grad_{\wv} LL(\wv)$)</li>
<li>Repeat above step until $LL(\wv)$ seems to be at a maximum.</li>
</ul>
<p>$$
      \begin{align*}
        \wv \leftarrow \wv + \alpha \grad_{\wv} LL(\wv)
      \end{align*}
$$</p>
<p>where $\alpha$ is a constant that affects the step size.</p>
<p>Remember that $\wv$ is a matrix of parameters, with, let's
say, columns corresponding to the values required for each $f$, of
which there are $K-1$.</p>
<p>We can work on the update formula and $\grad_{\wv} LL(\wv)$ one column at
a time</p>
<p>$$
    \begin{align*}
        \wv_k  \leftarrow \wv_k + \alpha \grad_{\wv_k} LL(\wv)
    \end{align*}
$$</p>
<p>and combine them at the end.</p>
<p>$$
    \begin{align*}
        \wv  \leftarrow \wv + \alpha (\grad_{\wv_1} LL(\wv),
        \grad_{\wv_2} LL(\wv), \ldots, \grad_{\wv_{K-1}} LL(\wv))
    \end{align*}
$$</p>
<p>Remembering that $\frac{\partial \log h(x)}{\partial x} = \frac{1}{h(x)}\frac{\partial h(x)}{x}$ and
that $p(C=k|\xv_n) = g_k(\xv_n)$</p>
<p>$$
      \begin{align*}
      LL(\wv) &amp; = \sum_{n=1}^N \sum_{k=1}^K  t_{n,k} \log p(C=k\,|\,\xv_n)\\
      &amp; = \sum_{n=1}^N \sum_{k=1}^K t_{n,k} \log g_k(\xv_n)\\
      \grad_{\wv_j} LL(\wv) &amp; = \sum_{n=1}^N \sum_{k=1}^K
      \frac{t_{n,k}}{g_k(\xv_n)} \grad_{\wv_j} g_k(\xv_n)
      \end{align*}
$$</p>
<p>It would be super nice if $\grad_{\wv_j} g_k(\xv_n)$
includes the factor $g_k(\xv_n)$ so that it will cancel
with the $g_k(\xv_n)$ in the denominator.</p>
<p>Can get this by defining</p>
<p>$$
    \begin{align*}
      f(\xv_n;\wv_k) &amp; = \ebx{k} \;\;\;\;\text{ so}\\
      g_k(\xv_n) &amp; = \frac{f(\xv_n;\wv_k)}{\sum_{m=1}^{K} f(\xv_n;\wv_m)}
    \end{align*}
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we can work on $\grad_{\wv_j} g_k(\xv_n)$.</p>
<p>$$
\begin{align*}
g_k(\xv_n) = \frac{\ebx{k}}{\sum_{m=1}^{K} \ebx{m}}
\end{align*}
$$</p>
<p>So</p>
<p>$$
    \begin{align*}
      \grad_{\wv_j} g_k(\xv_n) &amp; = \grad_{\wv_j} \left (\frac{\ebx{k}}{\sum_{m=1}^{K} \ebx{m}} \right )\\
    &amp; = \grad_{\wv_j} \left [ \left (\sum_{m=1}^{K} \ebx{m} \right )^{-1} \ebx{k} \right ] 
    \end{align*}
$$
Since
$$
\begin{align*}
\grad_{\wv_j} \ebx{k} &amp;= \begin{cases}
\xv_n \ebx{k}, &amp; \text{if } k=j\\
0 &amp; \text{otherwise}
\end{cases}
\end{align*}
$$
and
$$
\begin{align*}
\grad_{\wv_j} \sum_{m=1}^K-1 \ebx{m} &amp;= \xv_n \ebx{k}
\end{align*}
$$
then
$$
    \begin{align*}
      \grad_{\wv_j} g_k(\xv_n) &amp; = \grad_{\wv_j} \left (\frac{\ebx{k}}{\sum_{m=1}^{K} \ebx{m}} \right )\\
    &amp; = -1 \left (\sum_{m=1}^{K} \ebx{m} \right )^{-2} \xv_n \ebx{j}
    \ebx{k} + \left (\sum_{m=1}^{K} \ebx{m} \right )^{-1} 
    \begin{cases} \xv_n \ebx{k},&amp; \text{if} j=k\\ 0,&amp; \text{otherwise} \end{cases}\\
&amp; = -\frac{\ebx{k}}{\sum_{m=1}^{K} \ebx{m}}
  \frac{\ebx{j}}{\sum_{m=1}^{K} \ebx{j}} \xv_n +
  \begin{cases} \frac{\ebx{j}}{\sum_{m=1}^{K} \ebx{j}} \xv_n,&amp; \text{if} j=k\\ 0,&amp; \text{otherwise} \end{cases}\\
%&amp; = \frac{\ebx{k}}{\sum_{m=1}^{K} \ebx{m} } 
&amp; = - g_k(\xv_n) g_j(\xv_n) \xv_n + \begin{cases} g_j(\xv_n) \xv_n,^ \text{if} j=k\\ 0,&amp; \text{otherwise} \end{cases}\\
&amp; = g_k(\xv_n) (\delta_{jk} - g_j(\xv_n)) \xv_n
    \end{align*}
$$
where $\delta_{jk} = 1$ if $j=k$, 0 otherwise.</p>
<p>Substituting this back into the log likelihood expression, we get</p>
<p>$$
    \begin{align*}
      \grad_{\wv_j} LL(\wv) &amp; = \sum_{n=1}^N \sum_{k=1}^K \frac{t_{n,k}}{g_k(\xv_n)} \grad_{\wv_j} g_k(\xv_n)\\
    &amp; = \sum_{n=1}^N \sum_{k=1}^K \frac{t_{n,k}}{g_k(\xv_n)} \left (g_k(\xv_n) (\delta_{jk} - g_j(\xv_n)) \xv_n \right )\\
    &amp; = \sum_{n=1}^N \left ( \sum_{k=1}^K t_{n,k} \delta_{jk} -
  g_j(\xv_n) \sum_{k=1}^K t_{n,k} \right ) \xv_n\\
&amp; = \sum_{n=1}^N  (t_{n,j} - g_j(\xv_n)) \xv_n
    \end{align*}
$$</p>
<p>which results in this update rule for $\wv_j$</p>
<p>$$
    \begin{align*}
        \wv_j  \leftarrow \wv_j + \alpha \sum_{n=1}^N
        (t_{n,j} - g_j(\xv_n)) \xv_n
        \end{align*}
$$</p>
<p>How do we do this in python?  First, a summary of the derivation.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Derivation-Summary">Derivation Summary<a class="anchor-link" href="#Derivation-Summary">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$P(C=k\,|\,\xv_n)$ and the data likelihood we want to maximize:</p>
<p>$$
    \begin{align*}
      g_k(\xv_n) &amp; = P(C=k\,|\,\xv_n) =
      \frac{f(\xv_n;\wv_k)}{\sum_{m=1}^{K} f(\xv_n;\wv_m)}\\
      f(\xv_n;\wv_k) &amp; = \left \{ \begin{array}{ll} \ebx{k}; &amp; k &lt; K\\ 1;&amp; k = K \end{array} \right .\\
      L(\wv) &amp; = \prod_{n=1}^N \prod_{k=1}^K p(C=k\,|\, \xv_n) ^{t_{n,k}}\\
      &amp; = \prod_{n=1}^N \prod_{k=1}^K g_k(\xv_n)^{t_{n,k}}
    \end{align*}
$$</p>
<p>Gradient of log likelihood with respect to $\wv_j$:</p>
<p>$$         
    \begin{align*}
      \grad_{\wv_j} LL(\wv) &amp; = \sum_{n=1}^N \sum_{k=1}^K
      \frac{t_{n,k}}{g_k(\xv_n)} \grad_{\wv_j}
      g_k(\xv_n)\\
%&amp; = \sum_{n=1}^N \left ( \sum_{k=1}^K t_{n,k} \delta_{jk} -
%  g_j(\xv_n) \sum_{k=1}^K t_{n,k} \right )\\
&amp; = \sum_{n=1}^N \xv_n (t_{n,j} - g_j(\xv_n))
\end{align*}
$$</p>
<p>which results in this update rule for $\wv_j$</p>
<p>$$
    \begin{align*}
        \wv_j  \leftarrow \wv_j + \alpha \sum_{n=1}^N
        (t_{n,j} - g_j(\xv_n)) \xv_n
        \end{align*}
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Implementation-in-Python">Implementation in Python<a class="anchor-link" href="#Implementation-in-Python">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Update rule for $\wv_j$</p>
<p>$$
    \begin{align*}
      \wv_j  \leftarrow \wv_j + \alpha \sum_{n=1}^N
      (t_{n,j} - g_j(\xv_n)) \xv_n
    \end{align*}
$$</p>
<p>What are shapes of each piece?  Remember that whenever we are dealing with weighted sums of inputs, as we are here, add the constant 1 to the front of each sample.</p>
<ul>
<li>$\xv_n$ is $(D+1) \times 1$ ($+1$ for the constant 1 input)</li>
<li>$\wv_j$ is  $(D+1) \times 1$ </li>
<li>$t_{n,j} - g_j(\xv_n)$ is   a scalar</li>
</ul>
<p>So, this all works. But, notice the sum is over $n$, and each
term in the product as $n$ components, so we can do this as a dot product.</p>
<p>Let's remove the sum and replace subscript $n$ with
*.</p>
<p>$$
    \begin{align*}
      \wv_j  &amp;\leftarrow \wv_j + \alpha \sum_{n=1}^N
      (t_{n,j} - g_j(\xv_n)) \xv_n\\
      \wv_j  &amp;\leftarrow \wv_j + \alpha (t_{*,j} - g_j(\xv_*)) \xv_*\\
    \end{align*}
$$</p>
<p>What are shapes of each piece?</p>
<ul>
<li>$(t_{*,j} - g_j(\xv_*))$ is $N \times 1$</li>
<li>$\xv_* = X$ is  $N \times (D+1)$</li>
<li>$\wv_j$ is  $(D+1) \times 1$ </li>
</ul>
<p>So, this will work if we transpose $X$ and premultiply it and define
$g$ as a function that accepts $\Xv$.</p>
<p>$$
    \begin{align*}
%      \wv_j  &amp;\leftarrow \wv_j + \alpha (t_{*,j} -
%      g(\xv_*;\wv_j)) \xv_*\\
      \wv_j  &amp;\leftarrow \wv_j + \alpha \Xv^T (t_{*,j} -
      g_j(\Xv))
    \end{align*}
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's keep going...and try to make this expression work for
all of the $\wv$'s.
Playing with the subscripts again, replace $j$ with *.</p>
<p>$$
    \begin{align*}
      \wv_j  &amp;\leftarrow \wv_j + \alpha \Xv^T (t_{*,j} - g_j(\Xv))\\
      \wv_*  &amp;\leftarrow \wv_* + \alpha \Xv^T (t_{*,*} - g_*(\Xv))
    \end{align*}
$$</p>
<p>Now what are shapes?</p>
<ul>
<li>$\wv_* = \wv$ is  $(D+1) \times K$</li>
<li>$t_{*,*} = T$ is  $N \times K$</li>
<li>$g_*(\Xv)$ is   $N \times (K-1)$</li>
<li>$t_{*,*} - g_*(\Xv)$ is  $N \times K$</li>
<li>So, $\Xv^T (t_{*,*} - g_*(\Xv))$ is  $(D+1) \times K$</li>
<li>So, $\Xv^T (T - g(\Xv))$ is  $(D+1) \times K$</li>
</ul>
<p>Now our update equation for all $\wv$'s is</p>
<p>$$
    \begin{align*}
      \wv  &amp;\leftarrow \wv + \alpha \Xv^T (T - g(\Xv))
    \end{align*}
$$</p>
<p>We had defined, for $k = 1,\ldots, K$,</p>
<p>$$
    \begin{align*}
      f(\xv_n;\wv_k) &amp; =  \ebx{k} \\
        g_k(\xv) &amp;=  \dfrac{f(\xv;\wv_k)}{\sum_{m=1}^K f(\xv;\wv_m)}
      \end{align*}
$$</p>
<p>Changing these to handle all samples $\Xv$ and all parameters
$\wv$ we have</p>
<p>$$
    \begin{align*}
      f(\Xv;\wv) &amp; = e^{\Xv \wv}\\
      g(\Xv) &amp; = \frac{f(\Xv;\wv)}{\text{rowSums}(f(\Xv;\wv)}
    \end{align*}
$$</p>
<p>Given training data $\Xv$ ($N\times (D+1)$) and class
indicator variables $T$ ($N \times K)$), these expressions
can be performed with the following code.</p>
<p>First, we need a function to create indicator variables from the class labels, to get</p>
<p>$$
\begin{bmatrix}
1\\
2\\
2\\
1\\
3
\end{bmatrix}
\Rightarrow
\begin{bmatrix}
1 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0\\
0 &amp; 1 &amp; 0\\
1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1
\end{bmatrix}
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">makeIndicatorVars</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
    <span class="c1"># Make sure T is two-dimensiona. Should be nSamples x 1.</span>
    <span class="k">if</span> <span class="n">T</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">T</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>    
    <span class="k">return</span> <span class="p">(</span><span class="n">T</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">T</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">T</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">makeIndicatorVars</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># import pdb</span>
<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">w</span><span class="p">):</span>
    <span class="n">fs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>  <span class="c1"># N x K</span>
    <span class="n">denom</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">fs</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="c1"># pdb.set_trace()</span>
    <span class="n">gs</span> <span class="o">=</span> <span class="n">fs</span> <span class="o">/</span> <span class="n">denom</span>
    <span class="c1"># print(gs[:10,:])</span>
    <span class="k">return</span> <span class="n">gs</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The function <code>g</code> is sometimes called the <em>softmax</em> function.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now the updates to $\wv$ can be formed with code like</p>

<pre><code>TI = makeIndicatorVars(T)   
w = np.zeros((X.shape[1],TI.shape[1]))
alpha = 0.0001
for step in range(1000):
    gs = g(X,w)
    # Error does not involve the last column of indicator variables in TI nor gs
    w = w + alpha * np.dot(X.T, TI - gs) 

</code></pre>
<p>Here is code for applying linear logistic regression to the Parkinsons data from last lecture.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">loadParkinsonsData</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s1">&#39;parkinsons.data&#39;</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span><span class="s2">&quot;r&quot;</span><span class="p">)</span>
    <span class="n">header</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">readline</span><span class="p">()</span>
    <span class="n">names</span> <span class="o">=</span> <span class="n">header</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span>

    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="n">f</span> <span class="p">,</span><span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">,</span> <span class="n">usecols</span><span class="o">=</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">23</span><span class="p">))</span>

    <span class="n">targetColumn</span> <span class="o">=</span> <span class="n">names</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;status&quot;</span><span class="p">)</span>
    <span class="n">XColumns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">23</span><span class="p">)</span>
    <span class="n">XColumns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">XColumns</span><span class="p">,</span> <span class="n">targetColumn</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span><span class="n">XColumns</span><span class="p">]</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span><span class="n">targetColumn</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># to keep 2-d matrix form</span>
    <span class="n">names</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">&quot;status&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">names</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">names</span> <span class="o">=</span> <span class="n">loadParkinsonsData</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">names</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">standardize</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">mean</span><span class="p">,</span><span class="n">stds</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span><span class="o">/</span><span class="n">stds</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">qdalda</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">runParkLogReg</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">trainFraction</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span><span class="s2">&quot;r&quot;</span><span class="p">)</span>
    <span class="n">header</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">readline</span><span class="p">()</span>
    <span class="n">names</span> <span class="o">=</span> <span class="n">header</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span>

    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="n">f</span> <span class="p">,</span><span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">,</span> <span class="n">usecols</span><span class="o">=</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">23</span><span class="p">))</span>

    <span class="n">targetColumn</span> <span class="o">=</span> <span class="n">names</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;status&quot;</span><span class="p">)</span>
    <span class="n">XColumns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">23</span><span class="p">)</span>
    <span class="n">XColumns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">XColumns</span><span class="p">,</span> <span class="n">targetColumn</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="n">XColumns</span><span class="p">]</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="n">targetColumn</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="c1"># to keep 2-d matrix form</span>
    <span class="n">names</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">&quot;status&quot;</span><span class="p">)</span>

    <span class="n">healthyI</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">T</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">parkI</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">T</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">healthyI</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">healthyI</span><span class="p">)</span>
    <span class="n">parkI</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">parkI</span><span class="p">)</span>

    <span class="n">nHealthy</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">trainFraction</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">healthyI</span><span class="p">))</span>
    <span class="n">nPark</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">trainFraction</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">parkI</span><span class="p">))</span>
    <span class="n">rowsTrain</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">healthyI</span><span class="p">[:</span><span class="n">nHealthy</span><span class="p">],</span> <span class="n">parkI</span><span class="p">[:</span><span class="n">nPark</span><span class="p">]))</span>
    <span class="n">Xtrain</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">rowsTrain</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">Ttrain</span> <span class="o">=</span> <span class="n">T</span><span class="p">[</span><span class="n">rowsTrain</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">rowsTest</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">healthyI</span><span class="p">[</span><span class="n">nHealthy</span><span class="p">:],</span> <span class="n">parkI</span><span class="p">[</span><span class="n">nPark</span><span class="p">:]))</span>
    <span class="n">Xtest</span> <span class="o">=</span>  <span class="n">X</span><span class="p">[</span><span class="n">rowsTest</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">Ttest</span> <span class="o">=</span>  <span class="n">T</span><span class="p">[</span><span class="n">rowsTest</span><span class="p">,</span> <span class="p">:]</span>

    <span class="n">means</span><span class="p">,</span><span class="n">stds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">Xtrain</span> <span class="p">,</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">Xtrains</span> <span class="o">=</span> <span class="n">standardize</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">stds</span><span class="p">)</span>
    <span class="n">Xtests</span> <span class="o">=</span> <span class="n">standardize</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">stds</span><span class="p">)</span>
    
    <span class="n">Xtrains1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">Xtrains</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">Xtrains</span><span class="p">))</span>
    <span class="n">Xtests1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">Xtests</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">Xtests</span><span class="p">))</span>

    <span class="c1"># New stuff for linear logistic regression</span>

    <span class="n">TtrainI</span> <span class="o">=</span> <span class="n">makeIndicatorVars</span><span class="p">(</span><span class="n">Ttrain</span><span class="p">)</span>
    <span class="n">TtestI</span> <span class="o">=</span> <span class="n">makeIndicatorVars</span><span class="p">(</span><span class="n">Ttest</span><span class="p">)</span>

    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">Xtrains1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">TtrainI</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.0001</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
        <span class="n">gs</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">Xtrains1</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Xtrains1</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">TtrainI</span> <span class="o">-</span> <span class="n">gs</span><span class="p">)</span>
        <span class="n">likelihoodPerSample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">TtrainI</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">gs</span><span class="p">))</span> <span class="o">/</span> <span class="n">Xtrains</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">likelihood</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">likelihoodPerSample</span><span class="p">)</span>
        <span class="c1"># print(&quot;Step&quot;,step,&quot; l =&quot;,likelihoodPerSample)</span>
        
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot2grid</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">likelihood</span><span class="p">)</span>

    <span class="n">logregOutput</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">Xtrains1</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
    <span class="n">predictedTrain</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logregOutput</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">logregOutput</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">Xtests1</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
    <span class="n">predictedTestLR</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logregOutput</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;LogReg: Percent correct: Train </span><span class="si">{:.3g}</span><span class="s2"> Test </span><span class="si">{:.3g}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">percentCorrect</span><span class="p">(</span><span class="n">predictedTrain</span><span class="p">,</span> <span class="n">Ttrain</span><span class="p">),</span>
                                                                     <span class="n">percentCorrect</span><span class="p">(</span><span class="n">predictedTestLR</span><span class="p">,</span> <span class="n">Ttest</span><span class="p">)))</span>

    <span class="c1"># Previous QDA, LDA code</span>
    
    <span class="n">qda</span> <span class="o">=</span> <span class="n">qdalda</span><span class="o">.</span><span class="n">QDA</span><span class="p">()</span>
    <span class="n">qda</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">Ttrain</span><span class="p">)</span>
    <span class="n">qdaPredictedTrain</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">qda</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">)</span>
    <span class="n">qdaPredictedTest</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">qda</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   QDA: Percent correct: Train </span><span class="si">{:.3g}</span><span class="s2"> Test </span><span class="si">{:.3g}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">percentCorrect</span><span class="p">(</span><span class="n">qdaPredictedTrain</span><span class="p">,</span> <span class="n">Ttrain</span><span class="p">),</span>
                                                                     <span class="n">percentCorrect</span><span class="p">(</span><span class="n">qdaPredictedTest</span><span class="p">,</span> <span class="n">Ttest</span><span class="p">)))</span>

    <span class="n">lda</span> <span class="o">=</span> <span class="n">qdalda</span><span class="o">.</span><span class="n">LDA</span><span class="p">()</span>
    <span class="n">lda</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">Ttrain</span><span class="p">)</span>
    <span class="n">ldaPredictedTrain</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">qda</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">)</span>
    <span class="n">ldaPredictedTest</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">qda</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   LDA: Percent correct: Train </span><span class="si">{:.3g}</span><span class="s2"> Test </span><span class="si">{:.3g}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">percentCorrect</span><span class="p">(</span><span class="n">ldaPredictedTrain</span><span class="p">,</span> <span class="n">Ttrain</span><span class="p">),</span>
                                                                     <span class="n">percentCorrect</span><span class="p">(</span><span class="n">ldaPredictedTest</span><span class="p">,</span> <span class="n">Ttest</span><span class="p">)))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">subplot2grid</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">colspan</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Ttest</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">predictedTestLR</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">qdaPredictedTest</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ldaPredictedTest</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">percentCorrect</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">==</span><span class="n">t</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">))</span> <span class="o">*</span> <span class="mi">100</span>

<span class="k">def</span> <span class="nf">discQDA</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">stds</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">Sigma</span><span class="p">,</span> <span class="n">prior</span><span class="p">):</span>
    <span class="n">Xc</span> <span class="o">=</span> <span class="n">standardize</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">stds</span><span class="p">)</span> <span class="o">-</span> <span class="n">mu</span>
    <span class="k">if</span> <span class="n">Sigma</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">Sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">Sigma</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">det</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">Sigma</span><span class="p">)</span>        
    <span class="k">if</span> <span class="n">det</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinAlgError</span><span class="p">(</span><span class="s1">&#39;discQDA(): Singular covariance matrix&#39;</span><span class="p">)</span>
    <span class="n">SigmaInv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Sigma</span><span class="p">)</span>     <span class="c1"># pinv in case Sigma is singular</span>
    <span class="k">return</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">det</span><span class="p">)</span> \
           <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Xc</span><span class="p">,</span> <span class="n">SigmaInv</span><span class="p">)</span> <span class="o">*</span> <span class="n">Xc</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> \
           <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">runParkLogReg</span><span class="p">(</span><span class="s1">&#39;parkinsons.data&#39;</span><span class="p">,</span><span class="mf">0.8</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">runParkLogReg</span><span class="p">(</span><span class="s1">&#39;parkinsons.data&#39;</span><span class="p">,</span><span class="mf">0.8</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">runParkLogReg</span><span class="p">(</span><span class="s1">&#39;parkinsons.data&#39;</span><span class="p">,</span><span class="mf">0.8</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The code above is doing steepest ascent in the gradient of the log
likelihood.  Do we have a better way of doing this gradient ascent?</p>
<p>Hey, how about using Moller's Scaled Conjugate Gradient again?  Just
have to define the function being optimized and its gradient.  The
function to be optimized should be the negative of the log likelihood,
because SCG is designed to minimize the function.  And the gradient
function must also include this negative.  But with these negatives,
SCG will work fine for optimizing the weights in a linear logistic
regression classifier.</p>
<p>This is left for you to do.  You will get clues about how to do this from the neural network implementation of nonlinear logistic regression in the next set of notes.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are definitions of the log likehood and its gradient again.
$$
      \begin{align*}
      LL(\wv) &amp; = \sum_{n=1}^N \sum_{k=1}^K t_{n,k} \log g_k(\xv_n)\\
      \grad_{\wv_j} LL(\wv)  &amp; = \sum_{n=1}^N \xv_n (t_{n,j} - g_j(\xv_n))
      \end{align*}
$$</p>
<p>or, as matrices, and using the mean log likelihood,</p>
<p>$$
    \begin{align*}
    Y &amp;= g(\Xv)\\
    LL(\wv) &amp; = \text{np.mean}(T \cdot \log Y , \text{axis}=0) \\
      \grad_{\wv_j} LL(\wv) &amp; =  \Xv^T (T - Y) \;/\; (N\,K)
    \end{align*}
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">mlutilities</span> <span class="k">as</span> <span class="nn">ml</span>

<span class="k">def</span> <span class="nf">runParkLogReg2</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">trainFraction</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span><span class="s2">&quot;r&quot;</span><span class="p">)</span>
    <span class="n">header</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">readline</span><span class="p">()</span>
    <span class="n">names</span> <span class="o">=</span> <span class="n">header</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span>

    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="n">f</span> <span class="p">,</span><span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">,</span> <span class="n">usecols</span><span class="o">=</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">23</span><span class="p">))</span>

    <span class="n">targetColumn</span> <span class="o">=</span> <span class="n">names</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;status&quot;</span><span class="p">)</span>
    <span class="n">XColumns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">23</span><span class="p">)</span>
    <span class="n">XColumns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">XColumns</span><span class="p">,</span> <span class="n">targetColumn</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span><span class="n">XColumns</span><span class="p">]</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span><span class="n">targetColumn</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># to keep 2-d matrix form</span>
    <span class="n">names</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">&quot;status&quot;</span><span class="p">)</span>

    <span class="n">healthyI</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">T</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">parkI</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">T</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">healthyI</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">healthyI</span><span class="p">)</span>
    <span class="n">parkI</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">parkI</span><span class="p">)</span>

    <span class="n">nHealthy</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">trainFraction</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">healthyI</span><span class="p">))</span>
    <span class="n">nPark</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">trainFraction</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">parkI</span><span class="p">))</span>
    <span class="n">rowsTrain</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">healthyI</span><span class="p">[:</span><span class="n">nHealthy</span><span class="p">],</span> <span class="n">parkI</span><span class="p">[:</span><span class="n">nPark</span><span class="p">]))</span>
    <span class="n">Xtrain</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">rowsTrain</span><span class="p">,:]</span>
    <span class="n">Ttrain</span> <span class="o">=</span> <span class="n">T</span><span class="p">[</span><span class="n">rowsTrain</span><span class="p">,:]</span>
    <span class="n">rowsTest</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">healthyI</span><span class="p">[</span><span class="n">nHealthy</span><span class="p">:],</span> <span class="n">parkI</span><span class="p">[</span><span class="n">nPark</span><span class="p">:]))</span>
    <span class="n">Xtest</span> <span class="o">=</span>  <span class="n">X</span><span class="p">[</span><span class="n">rowsTest</span><span class="p">,:]</span>
    <span class="n">Ttest</span> <span class="o">=</span>  <span class="n">T</span><span class="p">[</span><span class="n">rowsTest</span><span class="p">,:]</span>

    <span class="n">means</span><span class="p">,</span><span class="n">stds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">Xtrains</span> <span class="o">=</span> <span class="n">standardize</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span><span class="n">means</span><span class="p">,</span><span class="n">stds</span><span class="p">)</span>
    <span class="n">Xtests</span> <span class="o">=</span> <span class="n">standardize</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span><span class="n">means</span><span class="p">,</span><span class="n">stds</span><span class="p">)</span>
    
    <span class="n">Xtrains1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">Xtrains</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">)),</span> <span class="n">Xtrains</span><span class="p">))</span>
    <span class="n">Xtests1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">Xtests</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">)),</span> <span class="n">Xtests</span><span class="p">))</span>

    <span class="c1"># New stuff for linear logistic regression</span>

    <span class="n">TtrainI</span> <span class="o">=</span> <span class="n">makeIndicatorVars</span><span class="p">(</span><span class="n">Ttrain</span><span class="p">)</span>
    <span class="n">TtestI</span> <span class="o">=</span> <span class="n">makeIndicatorVars</span><span class="p">(</span><span class="n">Ttest</span><span class="p">)</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">TtrainI</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">loglikelihood</span><span class="p">(</span><span class="n">warg</span><span class="p">,</span><span class="n">K</span><span class="p">):</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">warg</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">K</span><span class="p">))</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">Xtrains1</span><span class="p">,</span><span class="n">w</span><span class="p">)</span>
        <span class="c1"># print(w)</span>
        <span class="k">return</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">TtrainI</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">gradientloglikelihood</span><span class="p">(</span><span class="n">warg</span><span class="p">,</span><span class="n">K</span><span class="p">):</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">warg</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">K</span><span class="p">))</span>
        <span class="c1"># print(&#39;w&#39;,w)</span>
        <span class="n">N</span> <span class="o">=</span> <span class="n">Xtrains1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">Xtrains1</span><span class="p">,</span><span class="n">w</span><span class="p">)</span>
        <span class="c1"># print(&#39;Y&#39;,Y[:10,:])</span>
        <span class="c1"># print(&#39;Xtrains1&#39;,Xtrains1[:5,:])</span>
        <span class="c1"># print(&#39;TtrainI&#39;,TtrainI[:5,:])</span>
        <span class="c1"># print(&#39;dot&#39;,np.dot(Xtrains1.T,(Y-TtrainI)[:,:-1]))</span>
        <span class="c1"># print(&#39;N&#39;,N,&#39;K&#39;,K)</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Xtrains1</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">Y</span><span class="o">-</span><span class="n">TtrainI</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">N</span> <span class="o">*</span> <span class="n">K</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grad</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">Xtrains1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">TtrainI</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">resultSCG</span> <span class="o">=</span> <span class="n">ml</span><span class="o">.</span><span class="n">scg</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">loglikelihood</span><span class="p">,</span> <span class="n">gradientloglikelihood</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">nIterations</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">ftracep</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">wresult</span> <span class="o">=</span> <span class="n">resultSCG</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">wresult</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">K</span><span class="p">))</span>

    <span class="n">logregOutput</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">Xtrains1</span><span class="p">,</span><span class="n">w</span><span class="p">)</span>
    <span class="n">predictedTrain</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logregOutput</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">logregOutput</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">Xtests1</span><span class="p">,</span><span class="n">w</span><span class="p">)</span>
    <span class="n">predictedTest</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logregOutput</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;LogReg SCG: Percent correct: Train </span><span class="si">{:.3g}</span><span class="s2"> Test </span><span class="si">{:.3g}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">percentCorrect</span><span class="p">(</span><span class="n">predictedTrain</span><span class="p">,</span><span class="n">Ttrain</span><span class="p">),</span><span class="n">percentCorrect</span><span class="p">(</span><span class="n">predictedTest</span><span class="p">,</span><span class="n">Ttest</span><span class="p">)))</span>

    <span class="c1"># Previous QDA code</span>
    
    <span class="n">qda</span> <span class="o">=</span> <span class="n">qdalda</span><span class="o">.</span><span class="n">QDA</span><span class="p">()</span>
    <span class="n">qda</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">Ttrain</span><span class="p">)</span>
    <span class="n">qdaPredictedTrain</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">qda</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">)</span>
    <span class="n">qdaPredictedTest</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">qda</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   QDA: Percent correct: Train </span><span class="si">{:.3g}</span><span class="s2"> Test </span><span class="si">{:.3g}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">percentCorrect</span><span class="p">(</span><span class="n">qdaPredictedTrain</span><span class="p">,</span> <span class="n">Ttrain</span><span class="p">),</span>
                                                                     <span class="n">percentCorrect</span><span class="p">(</span><span class="n">qdaPredictedTest</span><span class="p">,</span> <span class="n">Ttest</span><span class="p">)))</span>

    <span class="n">lda</span> <span class="o">=</span> <span class="n">qdalda</span><span class="o">.</span><span class="n">LDA</span><span class="p">()</span>
    <span class="n">lda</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">Ttrain</span><span class="p">)</span>
    <span class="n">ldaPredictedTrain</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">qda</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">)</span>
    <span class="n">ldaPredictedTest</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">qda</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   LDA: Percent correct: Train </span><span class="si">{:.3g}</span><span class="s2"> Test </span><span class="si">{:.3g}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">percentCorrect</span><span class="p">(</span><span class="n">ldaPredictedTrain</span><span class="p">,</span> <span class="n">Ttrain</span><span class="p">),</span>
                                                                     <span class="n">percentCorrect</span><span class="p">(</span><span class="n">ldaPredictedTest</span><span class="p">,</span> <span class="n">Ttest</span><span class="p">)))</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">runParkLogReg2</span><span class="p">(</span><span class="s1">&#39;parkinsons.data&#39;</span><span class="p">,</span><span class="mf">0.8</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">runParkLogReg2</span><span class="p">(</span><span class="s1">&#39;parkinsons.data&#39;</span><span class="p">,</span><span class="mf">0.8</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
 


    </div>
  </div>

  </div>


  
    <footer class="footer hidden-print">
      <div class="container">
        <div class="col-md-4">
          <p>
            This website does not host notebooks, it only renders notebooks
            available on other websites.
          </p>
        </div>

        <div class="col-md-4">
          <p>
            Delivered by <a href="http://www.fastly.com/">Fastly</a>,
            Rendered by <a href="https://developer.rackspace.com/?nbviewer=awesome">Rackspace</a>
          </p>
          <p>
            nbviewer GitHub <a href="https://github.com/jupyter/nbviewer">repository</a>.
          </p>
        </div>

        <div class="col-md-4">
          
  
            
              <p>
                nbviewer version:
                <a href="https://github.com/jupyter/nbviewer/commit/67ee47e55fa697614e03aa060a86019f75f2ddca">
                  67ee47e
                </a>
              </p>
            
          
  
  <p>
    nbconvert version: <a href="https://github.com/jupyter/nbconvert/releases/tag/5.3.1">
      5.3.1
    </a>
  </p>
  

          
  
  
  <p>
    Rendered
    <span class='date' data-date='Sun, 25 Mar 2018 00:00:36 UTC' title='Sun, 25 Mar 2018 00:00:36 UTC'>(Sun, 25 Mar 2018 00:00:36 UTC)</span>
  </p>
  

        </div>
      </div>
    </footer>
  

  <script src="/static/components/bootstrap/js/bootstrap.min.js?v=5869c96cc8f19086aee625d670d741f9"></script>
  <script src="/static/components/headroom.js/dist/headroom.min.js?v=b0a311ea668f8e768ea375f4a7abb81c"></script>
  <script src="/static/components/headroom.js/dist/jQuery.headroom.min.js?v=f3a1bae118315d0c234afc74dc6aab71"></script>

  
  
  <script>
    $(function(){ $("#menubar").headroom({
      tolerance: 5,
      offset: 205,
      classes: {
        initial: "animated",
        pinned: "slideInDown",
        unpinned: "slideOutUp"
      }
    })});
  </script>


  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-52617120-5', 'auto');
    ga('send', 'pageview');
  </script>
  
  <script>
    require({
        paths: {
          moment: "/static/components/moment/min/moment.min.js?v=89f87298ad94aa1e6b92f42eb66da043"
        }
      }, ["moment"], function(moment){
      var date = $("footer .date"),
        m = moment(new Date(date.data('date'))),
        update = function(){ date.text(m.fromNow()); };
      setInterval(update, 61*1000);
      update();
      var w = $(window).scroll(function(event){
        $("body").toggleClass("scrolled", w.scrollTop() > 0);
      });
    });
  </script>

  <!--NEW RELIC Stop Perf Measurement-->
  
  <!--NEW RELIC End-->
</body>
</html>