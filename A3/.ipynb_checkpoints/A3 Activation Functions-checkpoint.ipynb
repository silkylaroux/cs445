{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damian Armijo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will make a new version of your ```NeuralNetwork``` class from the previous assignment. For this new version, define the activation function to be the Rectified Linear Unit (ReLU).\n",
    "\n",
    "You will compare the training and testing performances of networks with tanh and networks with the ReLU activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuralNetworkReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with the ```NeuralNetwork``` class defined in ```neuralnetworksA2.py```.  Define a new class named ```NeuralNetworkReLU``` that extends ```NeuralNetwork``` and simply defines new implementations of ```activation``` and ```activationDerivative``` that implement the ReLU activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a new function ```partition``` that is used as this example shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X = np.arange(10*2).reshape((10, 2))\n",
    "T = X[:, 0:1] * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def partition(X, T, seeding,shuffle = False):\n",
    "    nRows = X.shape[0]\n",
    "    rows = np.arange(nRows)\n",
    "    if(shuffle):\n",
    "        np.random.shuffle(rows)\n",
    "\n",
    "    nTrain = int(nRows * seeding)\n",
    "    trainRows = rows[:nTrain]\n",
    "    testRows = rows[nTrain:]\n",
    "    Xtrain, Ttrain = X[trainRows, :], T[trainRows, :]\n",
    "    Xtest, Ttest = X[testRows, :], T[testRows, :]\n",
    "    return Xtrain, Ttrain, Xtest, Ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1],\n",
       "       [ 2,  3],\n",
       "       [ 4,  5],\n",
       "       [ 6,  7],\n",
       "       [ 8,  9],\n",
       "       [10, 11],\n",
       "       [12, 13],\n",
       "       [14, 15],\n",
       "       [16, 17],\n",
       "       [18, 19]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0. ],\n",
       "       [ 0.2],\n",
       "       [ 0.4],\n",
       "       [ 0.6],\n",
       "       [ 0.8],\n",
       "       [ 1. ],\n",
       "       [ 1.2],\n",
       "       [ 1.4],\n",
       "       [ 1.6],\n",
       "       [ 1.8]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Ttrain, Xtest, Ttest = partition(X, T, 0.8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1],\n",
       "       [ 2,  3],\n",
       "       [ 4,  5],\n",
       "       [ 6,  7],\n",
       "       [ 8,  9],\n",
       "       [10, 11],\n",
       "       [12, 13],\n",
       "       [14, 15]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0. ],\n",
       "       [ 0.2],\n",
       "       [ 0.4],\n",
       "       [ 0.6],\n",
       "       [ 0.8],\n",
       "       [ 1. ],\n",
       "       [ 1.2],\n",
       "       [ 1.4]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ttrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16, 17],\n",
       "       [18, 19]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.6],\n",
       "       [ 1.8]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ttest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If ```shuffle=True``` is used as an argument, then the samples are randomly rearranged before the partitions are formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain, Ttrain, Xtest, Ttest = partition(X, T, 0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16, 17],\n",
       "       [10, 11],\n",
       "       [ 2,  3],\n",
       "       [18, 19],\n",
       "       [ 8,  9],\n",
       "       [ 6,  7],\n",
       "       [14, 15],\n",
       "       [12, 13]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6],\n",
       "       [1.4],\n",
       "       [0. ],\n",
       "       [1. ],\n",
       "       [1.6],\n",
       "       [0.8],\n",
       "       [0.4],\n",
       "       [1.8]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ttrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [4, 5]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0. ],\n",
       "       [ 0.4]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ttest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use the ```energydata_complete.csv``` data for the following comparisons.  Load this data using pandas, then create matrix $X$ using all columns except ```['date','Appliances', 'rv1', 'rv2']``` and create $T$ using just ```'Appliances'```.  Write python code that performs the following algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 30.          19.89        47.59666667 ...,   7.          63.           5.3       ]\n",
      " [ 30.          19.89        46.69333333 ...,   6.66666667  59.16666667\n",
      "    5.2       ]\n",
      " [ 30.          19.89        46.3        ...,   6.33333333  55.33333333\n",
      "    5.1       ]\n",
      " ..., \n",
      " [ 10.          25.5         46.59666667 ...,   3.66666667  25.33333333\n",
      "   13.26666667]\n",
      " [ 10.          25.5         46.99       ...,   3.83333333  26.16666667\n",
      "   13.23333333]\n",
      " [ 10.          25.5         46.6        ...,   4.          27.          13.2       ]]\n",
      "[[ 60]\n",
      " [ 60]\n",
      " [ 50]\n",
      " ..., \n",
      " [270]\n",
      " [420]\n",
      " [430]]\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "# Reading in file via pandas, and putting values into T(target) and X(inputs)\n",
    "data = pandas.read_csv('energydata_complete.csv')\n",
    "T = data[['Appliances']]\n",
    "T = np.array(T)\n",
    "X = data.drop(['date','Appliances', 'rv1', 'rv2'], axis=1)\n",
    "X = np.array(X)\n",
    "\n",
    "\n",
    "\n",
    "# Getting labels for T and X\n",
    "names = data.keys()\n",
    "Xnames = names[3:27]\n",
    "Tnames = names[0:2]\n",
    "Xnames = Xnames.insert(0, 'bias')\n",
    "\n",
    "print(X)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(A, B):\n",
    "    return np.sqrt(np.mean((A - B)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - For each of the two activation functions, ```tanh```, and ```ReLU```:\n",
    "      - For each hidden layer structure in [[u]*nl for u in [1, 2, 5, 10, 50] for nl in [1, 2, 3, 4, 5, 10]]:\n",
    "          - Repeat 10 times:\n",
    "              - Randomly partition the data into training set with 80% of samples and testing set with other 20%.\n",
    "              - Create a neural network using the given activation function and hidden layer structure.\n",
    "              - Train the network for 100 iterations.\n",
    "              - Calculate two RMS errors, one on the training partition and one on the testing partitions.\n",
    "          - Calculate the mean of the training and testing RMS errors over the 10 repetitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NeuralNetworkReLU as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def meanFromData(V):\n",
    "    meanTrain = V[:,0].mean()\n",
    "    meanTest = a[:,1].mean()\n",
    "    return meanTrain, meanTest;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'V' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-814c19125715>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#errors.append([rmse(Ttrain, nnet.use(Xtrain)), rmse(Ttest, nnet.use(Xtest))])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmeanTrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmeanTest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeanFromData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeanTrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmeanTest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'V' is not defined"
     ]
    }
   ],
   "source": [
    "#ReLU activation\n",
    "import pandas as pd\n",
    "errors = []\n",
    "hiddens = [0] + [[nu] * nl for nu in [1,2,5,10,50] for nl in [1, 2,3,4,5]]\n",
    "V = np.zeros\n",
    "for hids in hiddens:\n",
    "    for x in range(10):\n",
    "        Xtrain, Ttrain, Xtest, Ttest = partition(X, T, 0.8, shuffle=True)\n",
    "        nnet = nn.NeuralNetwork(Xtrain.shape[1], hids, Ttrain.shape[1])\n",
    "        nnet.train(Xtrain, Ttrain, 100)\n",
    "        #errors.append([rmse(Ttrain, nnet.use(Xtrain)), rmse(Ttest, nnet.use(Xtest))])\n",
    "        stack = [rmse(Ttrain, nnet.use(Xtrain)), rmse(Ttest, nnet.use(Xtest))]\n",
    "        V = np.vstack([V,stack])\n",
    "    meanTrain,meanTest = meanFromData(V)\n",
    "    errors.append([hids, meanTrain,meanTest])    \n",
    "errors = pd.DataFrame(errors)\n",
    "print(errors)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(errors.values[:, 1:], 'o-')\n",
    "plt.legend(('Train RMSE', 'Test RMSE'))\n",
    "plt.xticks(range(errors.shape[0]), hiddens, rotation=30, horizontalalignment='right')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will have to add steps in this algorithm to collect the results you need to make the following plot.\n",
    "\n",
    "Make a plot of the RMS errors versus the hidden layer structure.  On this plot include four curves, for the training and testing RMS errors for each of the two activation functions.  Label both axes and add a legend that identifies each curve.\n",
    "\n",
    "As always, discuss what you see.  What can you say about which activation function is best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading and Check-in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your notebook will be run and graded automatically. Test this grading process by first downloading [A3grader.tar](http://www.cs.colostate.edu/~anderson/cs445/notebooks/A3grader.tar) and extract `A3grader.py` from it. Run the code in the following cell to demonstrate an example grading session. You should see a perfect execution score of  60 / 60 if your functions and class are defined correctly. The remaining 40 points will be based on the results you obtain from the comparisons of hidden layer structures and the two activation functions applied to the energy data.\n",
    "\n",
    "For the grading script to run correctly, you must first name this notebook as `Lastname-A3.ipynb` with `Lastname` being your last name, and then save this notebook.  Your working director must also contain `neuralnetworksA2.py` and `mlutilities.py` from lecture notes.\n",
    "\n",
    "Combine your notebook, `neuralnetworkA2.py`, and `mlutilities.py` into one zip file or tar file.  Name your tar file `Lastname-A3.tar` or your zip file `Lastname-A3.zip`.  Check in your tar or zip file using the `Assignment 3` link in Canvas.\n",
    "\n",
    "A different, but similar, grading script will be used to grade your checked-in notebook. It will include other tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================= Code Execution =======================\n",
      "\n",
      "Extracting python code from notebook named 'Anderson-A3.ipynb' and storing in notebookcode.py\n",
      "Removing all statements that are not function or class defs or import statements.\n",
      "\n",
      "Testing  import neuralnetworksA2 as nn\n",
      "\n",
      "--- 5/5 points. The statement  import neuralnetworksA2 as nn  works.\n",
      "\n",
      "Testing nnet = nn.NeuralNetwork(1, 10, 1)\n",
      "\n",
      "--- 5/5 points. nnet correctly constructed\n",
      "\n",
      "Testing a = nnet.activation(-0.8)\n",
      "\n",
      "--- 5/5 points. activation of -0.6640367702678491 is correct.\n",
      "\n",
      "Testing da = nnet.activationDerivative(-0.664)\n",
      "\n",
      "--- 5/5 points. activationDerivative of 0.5591039999999999 is correct.\n",
      "\n",
      "Testing nnetrelu = NeuralNetworkReLU(1, 5, 1)\n",
      "\n",
      "--- 5/5 points. nnet correctly constructed\n",
      "\n",
      "Testing a = nnetrelu.activation(-0.8)\n",
      "\n",
      "--- 5/5 points. activation of 0.0 is correct.\n",
      "\n",
      "Testing a = nnetrelu.activation(1.8)\n",
      "\n",
      "--- 5/5 points. activation of 1.8 is correct.\n",
      "\n",
      "Testing da = nnetrelu.activationDerivative(0.0)\n",
      "\n",
      "--- 5/5 points. activationDerivative of 0.0 is correct.\n",
      "\n",
      "Testing da = nnetrelu.activationDerivative(5.5)\n",
      "\n",
      "--- 5/5 points. activationDerivative of 1.0 is correct.\n",
      "\n",
      "Testing X = np.arange(18).reshape((6, 3))\n",
      "        T = X[:,0:1] - X[:, 2:3]\n",
      "        Xtrain, Ttrain, Xtest, Ttest = partition(X, T, (0.5, 0.5), shuffle=False)\n",
      "\n",
      "--- 15/15 points. partition result is correct.\n",
      "\n",
      "A3 Execution Grade is 60/60\n",
      "\n",
      "============= Experiments with Energy Data =============\n",
      "\n",
      "--- _/20 points. Correct implementation of procedure for collecting RMSEs averaged over 10 runs.\n",
      "Comments:\n",
      "\n",
      "--- _/10 points. Correct plot with four curves for training and testing RMSEs for the two activation functions.\n",
      "                 Label y axis as \"RMSE\".\n",
      "                 Label x axis with hidden layer structures rotated a bit so they can be read.\n",
      "                 Include legend with brief label for each curve.\n",
      "Comments:\n",
      "\n",
      "--- _/10 points. Discussion of what you observe in the plot.\n",
      "Comments:\n",
      "\n",
      "A3 Notebook Grade is __ / 40\n",
      "\n",
      "A3 FINAL GRADE is __ / 100\n"
     ]
    }
   ],
   "source": [
    "%run -i A3grader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run additional experiments using different numbers of training iterations.  How do the relative performances of the three activation functions depend on numbers of training iterations?  This will earn one extra credit point.\n",
    "\n",
    "You may also earn an extra credit point by creating yet another version of the neural network class, called ```NeuralNetworkSwish``` and repeat the above comparisons.  You may set the constant $\\beta = 1$.  This is tricker than it sounds, because the Swish activation derivative requires the weighted sum as an argument, but our other two activation function derivatives did not."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
